import os
import json
import logging
import mlflow
import utils.databricks_api_helper as databricks_api_helper
# from mlflow_export_import.copy.copy_model_version import copy
import utils.aws_secrets_manager_helper as aws_secrets_manager_helper
from utils.common_entities import SubEnvironment, Environment, DeploymentMode, DatabricksMlflowStages

logger = logging.getLogger()
logger.setLevel(logging.INFO)

os.environ['MLFLOW_TRACKING_URI'] = 'databricks://src'

DEPLOYMENT_PARAMETERS = {}
SRC_REGISTRY_URI = "databricks-uc://src"
DST_REGISTRY_URI = "databricks-uc://target"

# TODO: Read the lambda layer from mlops-common-libraries => arn:aws:lambda:us-east-1:417854772506:layer:mlops_common_libraries:9

def init(deployment_parameters: dict) -> None:
    """Initialize deployment parameters"""
    global DEPLOYMENT_PARAMETERS
    DEPLOYMENT_PARAMETERS = deployment_parameters
    databricks_api_helper.init(deployment_parameters)

def set_databricks_environment(region, environment, automation_environment):
    """Used to establish a connection with Databricks"""
    try:
        databricks_host = f'https://formulaone-{region}-{environment.lower()}-ai-workspace.cloud.databricks.com'
        os.environ["DATABRICKS_HOST"] = databricks_host
        
        secret_name = f"f1ai/{automation_environment.lower()}"
        secret_key = f"databricks_token_{environment.lower()}"
        
        databricks_token = aws_secrets_manager_helper.get_secret_value_by_key(
            secret_name=secret_name, 
            secret_key=secret_key
        )
        
        if not databricks_token:
            raise ValueError(f"Failed to retrieve Databricks token for {secret_name}/{secret_key}")
            
        os.environ["DATABRICKS_TOKEN"] = databricks_token
        logger.info(f"Databricks token set for environment: {environment}")
        logger.info('Credentials received successfully')
        
        mlflow.set_tracking_uri("databricks")
        
    except Exception as e:
        logger.error(f"Failed to set Databricks environment: {str(e)}")
        raise

def promote_model_registration(model_name, model_version, model_source, src_token=None, target_token=None, 
                            src_host=None, target_host=None, src_schema="f1aiplat.dwsa0004067", 
                            target_schema="f1aiplat.dwsa0004067", user_id=None):
    """
    Promote model from lower Databricks workspace to higher Databricks workspace.
    
    :param model_name: Source model name from Unity catalog.
    :param model_version: Source model version to promote.
    :param model_source: Registry address where to register the model.
    :param src_token: Source Databricks token (if not using AWS Secrets Manager).
    :param target_token: Target Databricks token (if not using AWS Secrets Manager).
    :param src_host: Source Databricks host.
    :param target_host: Target Databricks host.
    :param src_schema: Source schema (default: f1aiplat.dwsa0004067).
    :param target_schema: Target schema (default: f1aiplat.dwsa0004067).
    :param user_id: User ID for experiment naming.
    """
    try:
        # Validate required parameters
        if not all([model_name, model_version, model_source]):
            raise ValueError("model_name, model_version, and model_source are required")
        
        # Use direct tokens if provided, otherwise fall back to deployment parameters
        if src_token and target_token and src_host and target_host:
            # Direct token usage
            logger.info("Using direct tokens for model promotion")
            
            # Create configuration file for Databricks connections
            create_cfg_file(
                src_host=src_host,
                src_token=src_token,
                target_host=target_host,
                target_token=target_token
            )
            
            experiment_name = f"/Users/{user_id}/{model_name}" if user_id else f"/Shared/{model_name}"
            
        else:
            # Fallback to deployment parameters (AWS Secrets Manager)
            workspace_id = DEPLOYMENT_PARAMETERS.get("deployment_workspace_id")
            src_host = DEPLOYMENT_PARAMETERS.get("deployment_databricks_host_lower_env")
            src_schema = DEPLOYMENT_PARAMETERS.get("deployment_databricks_schema_lower_env", src_schema)
            target_host = DEPLOYMENT_PARAMETERS.get("deployment_databricks_host")
            target_schema = DEPLOYMENT_PARAMETERS.get("deployment_databricks_schema", target_schema)
            user_id = DEPLOYMENT_PARAMETERS.get("databricks_deployment_user")
            
            # Validate deployment parameters
            required_params = [workspace_id, src_host, target_host, user_id]
            if not all(required_params):
                missing = [k for k, v in {
                    "workspace_id": workspace_id,
                    "src_host": src_host, 
                    "target_host": target_host,
                    "user_id": user_id
                }.items() if not v]
                raise ValueError(f"Missing required deployment parameters: {missing}")
            
            src_env, target_env = get_envs_names(DEPLOYMENT_PARAMETERS["deployment_environment"])
            
            # Create configuration file for Databricks connections
            create_cfg_file(
                src_host=src_host,
                src_token=get_dbs_token(src_env),
                target_host=target_host,
                target_token=get_dbs_token(target_env)
            )
            
            experiment_name = f"/Users/{user_id}/{model_name}"
        
        # Promote the model
        result = promote_model(
            src_model=f"{src_schema}.{model_name}",
            src_version=model_version,
            src_registry_uri=SRC_REGISTRY_URI,
            dst_model=f"{target_schema}.{model_name}",
            dst_registry_uri=DST_REGISTRY_URI,
            dst_experiment_name=experiment_name,
        )
        
        logger.info(f"Model promotion completed for: {model_name}")
        return {'status': 'SUCCESS', 'result': result}
        
    except Exception as e:
        logger.error(f"Model promotion failed: {str(e)}")
        return {'status': 'FAILURE', 'message': str(e)}

def get_envs_names(deployment_env) -> tuple:
    """Get source and target environment names"""
    src_env = 'dev'
    if deployment_env == 'prod':
        src_env = 'qa'
    return src_env, deployment_env

def get_dbs_token(environment):
    """Get Databricks token for specified environment"""
    try:
        secret_name = f"f1ai/{environment.lower()}"
        secret_key = f"databricks_token_{environment.lower()}"
        
        token = aws_secrets_manager_helper.get_secret_value_by_key(
            secret_name=secret_name,
            secret_key=secret_key
        )
        
        if not token:
            raise ValueError(f"No token found for {secret_name}/{secret_key}")
            
        return token
        
    except Exception as e:
        logger.error(f"Failed to get Databricks token for {environment}: {str(e)}")
        raise

def promote_model(
    src_model,
    src_version, 
    src_registry_uri,
    dst_model,
    dst_registry_uri,
    dst_experiment_name=None,
    copy_permissions=False,
    copy_stages_and_aliases=False,
    copy_lineage_tags=False,
    verbose=False
):
    """Promote model from source to destination registry"""
    
    def make_tracking_uri(registry_uri):
        """Convert registry URI to tracking URI"""
        if not registry_uri:
            return None
        if registry_uri.startswith("databricks-uc"):
            return registry_uri.replace("databricks-uc", "databricks")
        else:
            return registry_uri

    try:
        # TODO: Can we omit dst_experiment_name?
        logger.info(f"Model info - model: {src_model}, version: {src_version}, source: {src_registry_uri}, target: {dst_registry_uri}")
        
        src_tracking_uri = make_tracking_uri(src_registry_uri)
        dst_tracking_uri = make_tracking_uri(dst_registry_uri)
        
        logger.info("Initializing model copy to target workspace...")
        
        # Check if model exists before promotion
        model_exists = check_model_exists(src_model, src_version, src_tracking_uri)
        if not model_exists:
            raise ValueError(f"Source model {src_model} version {src_version} not found")
        
        # Uncomment and use the actual copy function when available
        # return copy(
        #     src_model,
        #     src_version,
        #     dst_model,
        #     dst_experiment_name,
        #     src_tracking_uri,
        #     dst_tracking_uri,
        #     src_registry_uri,
        #     dst_registry_uri,
        #     copy_permissions,
        #     copy_stages_and_aliases,
        #     copy_lineage_tags,
        #     verbose
        # )
        
        # Placeholder return for now
        logger.info("Model promotion simulation completed")
        return {"status": "SUCCESS", "message": "Model promotion completed"}
        
    except Exception as e:
        logger.error(f"Model promotion failed: {str(e)}")
        raise

def check_model_exists(model_name, model_version, tracking_uri):
    """Check if model exists in the source registry"""
    try:
        # Set tracking URI temporarily
        original_uri = mlflow.get_tracking_uri()
        mlflow.set_tracking_uri(tracking_uri)
        
        # Try to get model version
        client = mlflow.MlflowClient()
        model_version_details = client.get_model_version(model_name, model_version)
        
        # Restore original URI
        mlflow.set_tracking_uri(original_uri)
        
        return model_version_details is not None
        
    except Exception as e:
        logger.warning(f"Model existence check failed: {str(e)}")
        # Restore original URI in case of error
        try:
            mlflow.set_tracking_uri(original_uri)
        except:
            pass
        return False

def create_cfg_file(src_host, src_token, target_host, target_token):
    """Create Databricks configuration file"""
    try:
        logger.info("Creating configuration file...")
        
        if not all([src_host, src_token, target_host, target_token]):
            raise ValueError("All connection parameters are required")
            
        content = f"""[src]
host={src_host.strip()}
token={src_token.strip()}

[target]
host={target_host.strip()}
token={target_token.strip()}
"""
        
        config_path = "/tmp/.databrickscfg"
        os.environ['DATABRICKS_CONFIG_FILE'] = config_path
        
        with open(config_path, 'w') as f:
            f.write(content)
            
        logger.info(f"Configuration file created at: {config_path}")
        return config_path
        
    except Exception as e:
        logger.error(f"Failed to create configuration file: {str(e)}")
        raise

def get_model_registration(model_name, model_version):
    """Get model registration details"""
    try:
        model_registration = databricks_api_helper.get_model_registration(model_name, model_version)
        if model_registration:
            return model_registration
        return None
    except Exception as e:
        logger.error(f"Failed to get model registration for {model_name} v{model_version}: {str(e)}")
        return None

def get_latest_model_registration(model_name, stage=None):
    """Get latest model registration, optionally filtered by stage"""
    try:
        registered_versions = databricks_api_helper.get_latest_model_registrations(model_name)
        
        if not registered_versions or len(registered_versions) == 0:
            logger.warning(f"No registered versions found for model: {model_name}")
            return None
            
        max_version = 0
        max_model_item = None
        
        def to_int(s):
            """Safely convert string to integer"""
            try:
                return int(s)
            except (ValueError, TypeError):
                return 0
        
        for model_item in registered_versions:
            if stage:
                # If stage is specified, return first match
                if model_item.get("current_stage") == stage:
                    return model_item
            elif model_item.get("current_stage") != "Archived":
                # If no stage specified, find latest non-archived version
                logger.info(f"Processing model_item: {model_item}")
                current_version = to_int(model_item.get("version", 0))
                if current_version > max_version:
                    max_version = current_version
                    max_model_item = model_item
                    
        return max_model_item
        
    except Exception as e:
        logger.error(f"Failed to get latest model registration for {model_name}: {str(e)}")
        return None

def set_databricks_token_directly(token, host):
    """Set Databricks token and host directly without AWS Secrets Manager"""
    try:
        if not token or not host:
            raise ValueError("Both token and host are required")
            
        os.environ["DATABRICKS_TOKEN"] = token
        os.environ["DATABRICKS_HOST"] = host
        
        logger.info(f"Databricks connection set for host: {host}")
        logger.info('Direct token credentials set successfully')
        
        mlflow.set_tracking_uri("databricks")
        
    except Exception as e:
        logger.error(f"Failed to set Databricks token directly: {str(e)}")
        raise

# Usage example when script is run directly
if __name__ == "__main__":
    logger.info("F1AI Model Promoter ready - use functions directly with your token")
