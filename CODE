import logging
from typing import List, Dict
import jsonschema
import json

from utils.aws_sagemaker_pipeline import AwsSagemakerPipelineBase
import utils.aws_sagemaker_pipeline_helper as aws_sagemaker_pipeline_helper
import utils.aws_sagemaker_processing_job_helper as aws_sagemaker_processing_job_helper
import utils.aws_secrets_manager_helper as aws_secrets_manager_helper
import utils.aws_event_bridge_helper as aws_event_bridge_helper
import utils.aws_sns_helper as aws_sns_helper

logger = logging.getLogger()
logger.setLevel(logging.INFO)

class AwsSagemakerProcessingJobPipeline(AwsSagemakerPipelineBase):

    def __init__(self, deployment_parameters: dict):

        super().__init__(deployment_parameters)

        # Initialize helper for Processing Pipelines
        aws_sagemaker_processing_job_helper.init(self.deployment_parameters)
        aws_secrets_manager_helper.init()

    def standardize_pipeline_name(self, pipeline_name: str) -> str:
        
        # Add workspace_id prefix to the pipeline name
        return f"{self.deployment_parameters['deployment_workspace_id'].lower()}-{pipeline_name}".replace('_', '-')

    def create_sagemaker_tags(self) -> List[Dict[str, str]]:

        sagemaker_tags = self.create_tags()
        sagemaker_tag_list = []

        for tag in sagemaker_tags:
            sagemaker_tag_list.append({"Key": tag, "Value": sagemaker_tags[tag]})

        logger.info(f"Tags List {sagemaker_tag_list=}")

        return sagemaker_tag_list

    def create_or_update_pipeline(self, pipeline_name: str, pipeline_definition: dict) -> dict:
        """
        Create or update the SageMaker pipeline. Implementation for Processing Pipelines.

        Args:
            pipeline_name (str): The name of the processing pipeline.
            pipeline_definition (dict): The JSON definition of the pipeline.

        Returns:
            dict: A dictionary containing the pipeline name or an error message.
        """

        try:
            # Validate Pipeline name and determine mode (create or update)
            pipeline_name = self.standardize_pipeline_name(pipeline_name)

            # Check if the pipeline name is new or existing        
            mode = 'CREATE' if aws_sagemaker_pipeline_helper.check_if_pipeline_name_exists(pipeline_name) else 'UPDATE'

            logger.info(f"User requesting to '{mode}' pipeline '{pipeline_name}'.")

            final_pipeline_steps = []

            # Get tags for the SageMaker Model, Pipeline and Batch Transform Job that will be created
            tags = self.create_sagemaker_tags()

            # Process each step in the pipeline definition
            for step in pipeline_definition['Steps']:

                if step["Type"] == "Processing":

                    allowed_schema_preprocessing_step = aws_sagemaker_processing_job_helper.get_allowed_schema()
                    jsonschema.validate(
                        instance=step, 
                        schema=allowed_schema_preprocessing_step
                    )

                    # Update ImageURI 
                    image_name = step['Arguments']['AppSpecification']['ImageUri']                    

                    if ':' not in image_name:
                        raise ValueError(f"Invalid ImageUri format: {image_name}")

                    image_name, image_tag_prefix = image_name.split(":")
                    ecr_repo = f"f1ai/applications/{self.deployment_parameters['deployment_workspace_id'].lower()}_{image_name}"
                    step['Arguments']['AppSpecification']['ImageUri'] = aws_sagemaker_processing_job_helper.get_full_image_uri(
                        repository_name=ecr_repo, 
                        image_tag_prefix=image_tag_prefix
                    )

                    # Get the default schema of the Processing Step and then update with user data
                    default_schema_processing_step = aws_sagemaker_processing_job_helper.get_default_schema(
                        sagemaker_processing_step_name=step['Name'], 
                        tags=tags
                    )
                    # Merge Schemas to create final step
                    final_processing_step = aws_sagemaker_pipeline_helper.merge_schemas(
                        default_step_dict=default_schema_processing_step,
                        user_step_dict=step
                    )

                    # Add platform parameters. TODO: Check if this necessary
                    final_processing_step['Arguments']["Environment"].update({
                        "f1ai_execution_pipeline_key": {"Get": "Execution.PipelineExecutionId"}
                    })
                    
                    # Add to Final list of Steps
                    final_pipeline_steps.append(final_processing_step)

            # Pretty print the final pipeline steps for debugging
            logger.debug(f"Final Pipeline Steps:\n{json.dumps(final_pipeline_steps, indent=4)}")

            # Get Pipeline Parameters
            pipeline_parameters = pipeline_definition.get('Parameters', [])

            if mode == 'UPDATE':
                
                logger.info('Updating the pipeline..')

                # Update Pipeline using post-processed Pipeline Definition 
                aws_sagemaker_pipeline_helper.update_processing_pipeline(
                    pipeline_name,
                    final_pipeline_steps,
                    pipeline_parameters
                )

                logger.info(f"Pipeline '{pipeline_name}' updated successfully.")
            
            else:
                
                logger.info('Creating a new pipeline.')

                # Create Pipeline using post-processed Pipeline Definition 
                aws_sagemaker_pipeline_helper.create_processing_pipeline(
                    pipeline_name,
                    final_pipeline_steps,
                    tags,
                    pipeline_parameters
                )
                
                logger.info(f"Pipeline '{pipeline_name}' created successfully.")

            # Handle SNS Topics
            for status in self.status_list:
                topic_name = self.get_pipeline_topic_name(pipeline_name, status)

                if mode == 'UPDATE':
                    # Check if the topic exists before creating
                    if not aws_sns_helper.topic_exists(topic_name):
                        aws_sns_helper.create_topic(
                            topic_name,
                            tags
                        )
                        logger.info(f"SNS topic '{topic_name}' created successfully.")
                    
                    else:
                        logger.info(f"SNS topic '{topic_name}' already exists. Skipping creation.")
                
                else:
                    
                    # Create SNS topic without checking
                    aws_sns_helper.create_topic(
                        topic_name,
                        tags
                    )
                    logger.info(f"SNS topic '{topic_name}' created successfully.")

            # Define EventBridge event pattern
            event_pattern = aws_sagemaker_pipeline_helper.get_pipeline_status_change_event_pattern()
            
            if mode == 'UPDATE':
                
                # Check if the EventBridge rule exists
                if aws_event_bridge_helper.check_if_rule_exists(pipeline_name):
                    logger.info(f"EventBridge rule '{pipeline_name}' already exists. Retrieving ARN.")
                    rule_arn = aws_event_bridge_helper.get_rule_arn(pipeline_name)
                else:
                    # Create EventBridge rule since it does not exist
                    rule_arn = aws_event_bridge_helper.add_rule(pipeline_name, event_pattern)
                    logger.info(f"EventBridge rule '{pipeline_name}' created successfully. ARN: {rule_arn}")
            
            else:
                # Create EventBridge rule without checking
                rule_arn = aws_event_bridge_helper.add_rule(pipeline_name, event_pattern)
                logger.info(f"EventBridge rule '{pipeline_name}' created successfully. ARN: {rule_arn}")

            rule_name = rule_arn.split('/')[-1]
            target_arn = (
                f"arn:aws:lambda:{self.deployment_parameters['deployment_region']}:"
                f"{self.deployment_parameters['deployment_aws_account_number']}:function:aws-common-pipeline-notification"
            )
            
            if mode == 'UPDATE':
                # Check if the target already exists
                if aws_event_bridge_helper.check_if_rule_target_exists(rule_name, target_arn):
                    logger.info(f"EventBridge rule target '{target_arn}' already exists. Skipping addition.")
                else:
                    aws_event_bridge_helper.add_rule_target('common-pipeline-notification', rule_name, target_arn)
                    logger.info(f"EventBridge rule target '{target_arn}' added successfully.")
            else:
                # Add rule target without checking
                aws_event_bridge_helper.add_rule_target('common-pipeline-notification', rule_name, target_arn)
                logger.info(f"EventBridge rule target '{target_arn}' added successfully.")

            return {"pipeline_name": pipeline_name}

        except jsonschema.ValidationError as ve:
            logger.error(f"Schema validation error: {ve.message}")
            raise RuntimeError(f"Schema validation error: {ve.message}") from ve

        except Exception as e:
            logger.error(f"Error creating/updating SageMaker Pipeline '{pipeline_name}': {str(e)}")
            raise RuntimeError(f"Error creating/updating SageMaker Pipeline '{pipeline_name}': {str(e)}") from e

(venv) sagemaker-user@default:~$ python3 f1_ai_mlops_baseline/python_scripts/utils/f1ai_model_promoter.py 
Traceback (most recent call last):
  File "/home/sagemaker-user/f1_ai_mlops_baseline/python_scripts/utils/f1ai_model_promoter.py", line 2, in <module>
    from f1ai_client import f1ai_client
ModuleNotFoundError: No module named 'f1ai_client'
(venv) sagemaker-user@default:~$ pip install f1ai_client
ERROR: Could not find a version that satisfies the requirement f1ai_client (from versions: none)
ERROR: No matching distribution found for f1ai_client
(venv) sagemaker-user@default:~$ 
